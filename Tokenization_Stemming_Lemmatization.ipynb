{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9mWOVJc8F55Hny8oh5vl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prateek-Poonia/Natural-Language-Processing/blob/main/Tokenization_Stemming_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCqGy3MgIELM",
        "outputId": "e7926f30-6f4c-4be8-f222-9212529c19bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVVy9PPnIwrb",
        "outputId": "8738aa45-29ec-41a8-b522-68c23cb33b85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Hello everyone, my name is Prateek Poonia. I am pursuing BTech CSE from LPU. My specialization is machine learning\""
      ],
      "metadata": {
        "id": "AQ1sAhWPIHlO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_tokenization = sent_tokenize(corpus)\n",
        "print(sentence_tokenization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-zgQBtIe1B",
        "outputId": "7df5f0a9-bda3-4fcb-c2a6-57804ef0fe84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone, my name is Prateek Poonia.', 'I am pursuing BTech CSE from LPU.', 'My specialization is machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sentence_tokenization:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOJJfL7LWrlZ",
        "outputId": "9dfb6922-6097-4887-91f7-cf1fc4760ab4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone, my name is Prateek Poonia.\n",
            "I am pursuing BTech CSE from LPU.\n",
            "My specialization is machine learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenization = word_tokenize(corpus)\n",
        "print(word_tokenization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZgD9VzkIiwM",
        "outputId": "f1166b0e-82ab-45ed-c129-f2a9a76be5ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', ',', 'my', 'name', 'is', 'Prateek', 'Poonia', '.', 'I', 'am', 'pursuing', 'BTech', 'CSE', 'from', 'LPU', '.', 'My', 'specialization', 'is', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING**"
      ],
      "metadata": {
        "id": "DMc6PUHQbfVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using porterStemmer\n",
        "\n",
        "words = [\"Stemming\",\"helping\",\"Congratulations\",\"runing\",\"fighting\",\"singing\",\"Developing\"]\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for word in words:\n",
        "  print(word+\"------>\"+ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ5RwG5FJit4",
        "outputId": "3a4ace5b-83b6-4eab-80d4-e4e08ac248c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming------>stem\n",
            "helping------>help\n",
            "Congratulations------>congratul\n",
            "runing------>rune\n",
            "fighting------>fight\n",
            "singing------>sing\n",
            "Developing------>develop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using RegexpStemmer\n",
        "\n",
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "rs = RegexpStemmer(\"ing|s|es|able\", min = 4)\n",
        "\n",
        "for word in words:\n",
        "  print(word+\"---->\"+rs.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ykkwObcHZl",
        "outputId": "a03d2b69-08f2-4b35-d49b-6a364b57b2c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming---->Stemm\n",
            "helping---->help\n",
            "Congratulations---->Congratulation\n",
            "runing---->run\n",
            "fighting---->fight\n",
            "singing---->\n",
            "Developing---->Develop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using SnowballStemmer\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sbs = SnowballStemmer(\"english\")\n",
        "\n",
        "for word in words:\n",
        "  print(word+\"---->\"+sbs.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTycJo3feW0B",
        "outputId": "e4c30315-cc87-4dd7-d280-9c914471be80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming---->stem\n",
            "helping---->help\n",
            "Congratulations---->congratul\n",
            "runing---->rune\n",
            "fighting---->fight\n",
            "singing---->sing\n",
            "Developing---->develop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sbs.stem(\"fairly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UJDQRSz4fxq3",
        "outputId": "4871d957-b8c2-40f4-b2bf-5308cade8e02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fair'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('fairly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UXxU6fVkgBAw",
        "outputId": "b99ea87b-2aa2-4181-8a39-4bceec693870"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairli'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATIZATION**"
      ],
      "metadata": {
        "id": "NtbJ8G57RfhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q77xN7VISURK",
        "outputId": "f9bb22a6-66e7-4ae3-f0cc-1109d92ffd79"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "for word in words:\n",
        "  print(word+\"--->\"+lm.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-b3lK4ygEoK",
        "outputId": "4a31c5e7-a195-4abf-e664-28c6ae90709a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming--->Stemming\n",
            "helping--->help\n",
            "Congratulations--->Congratulations\n",
            "runing--->run\n",
            "fighting--->fight\n",
            "singing--->sing\n",
            "Developing--->Developing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPGtRfligTmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}