{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmjBIDpVS83SAortHk5DZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prateek-Poonia/Natural-Language-Processing/blob/main/Lemmatization_Bag_of_words_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k-NqjpTfs_gR"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\""
      ],
      "metadata": {
        "id": "pjCHDBuWt4dj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "icbSddyRt9jw",
        "outputId": "056e0f31-ca50-4aa1-8bea-c6b4c3edc18d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "o44Pt0zkt-YJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization --> converts paragraph -sentences-words\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7KdgPycuJSd",
        "outputId": "51fb2a1e-7a65-43ff-db99-27c0850871e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDeLKRQgujvu",
        "outputId": "f03d0b0e-eadc-465d-95e2-07faa6f8e1f9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLTK is a leading platform for building Python programs to work with human language data.',\n",
              " 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.',\n",
              " 'NLTK is available for Windows, Mac OS X, and Linux.',\n",
              " 'Best of all, NLTK is a free, open source, community-driven project.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAdzzCK8uoDY",
        "outputId": "2664c8ac-3e3f-42d7-a4c7-fa9ef2715841"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "stemmer=PorterStemmer()\n"
      ],
      "metadata": {
        "id": "7jO1ppzd3pY5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('programming')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pWh-Csma4GLA",
        "outputId": "baf4c7ae-1fb4-49a7-92ec-f1b1763fb3ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'program'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Ig9ftU7P4Mic"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jiRMc-Qa5Ww0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgB8qdX14Xok",
        "outputId": "1b9856e5-e7eb-4a3f-d481-af4a973f29ef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('reasoning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mNuKO5tm4aMV",
        "outputId": "9b7fe48b-b360-460f-ddad-fa27b7fb090a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reasoning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "IMW2mtVf4h5L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRHPIWqD5w1s",
        "outputId": "e8663eec-d121-4845-ca74-c6389a49b9c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "  review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  review=review.lower()\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "ifNPWjpK4rDY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmAOVQI54xct",
        "outputId": "13d0749a-5f3c-4534-dbd1-6b173f18efcf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nltk is a leading platform for building python programs to work with human language data ',\n",
              " 'it provides easy to use interfaces to over    corpora and lexical resources such as wordnet  along with a suite of text processing libraries for classification  tokenization  stemming  tagging  parsing  and semantic reasoning  wrappers for industrial strength nlp libraries  and an active discussion forum thanks to a hands on guide introducing programming fundamentals alongside topics in computational linguistics  plus comprehensive api documentation  nltk is suitable for linguists  engineers  students  educators  researchers  and industry users alike ',\n",
              " 'nltk is available for windows  mac os x  and linux ',\n",
              " 'best of all  nltk is a free  open source  community driven project ']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElqRJNL77SKP",
        "outputId": "656fd907-0048-4b94-89c9-9b805bd14743"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKprVCl56htv",
        "outputId": "0b7eac45-a94d-4705-a010-0bebdb936ebd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk\n",
            "lead\n",
            "platform\n",
            "build\n",
            "python\n",
            "program\n",
            "work\n",
            "human\n",
            "languag\n",
            "data\n",
            "provid\n",
            "easi\n",
            "use\n",
            "interfac\n",
            "corpora\n",
            "lexic\n",
            "resourc\n",
            "wordnet\n",
            "along\n",
            "suit\n",
            "text\n",
            "process\n",
            "librari\n",
            "classif\n",
            "token\n",
            "stem\n",
            "tag\n",
            "pars\n",
            "semant\n",
            "reason\n",
            "wrapper\n",
            "industri\n",
            "strength\n",
            "nlp\n",
            "librari\n",
            "activ\n",
            "discuss\n",
            "forum\n",
            "thank\n",
            "hand\n",
            "guid\n",
            "introduc\n",
            "program\n",
            "fundament\n",
            "alongsid\n",
            "topic\n",
            "comput\n",
            "linguist\n",
            "plu\n",
            "comprehens\n",
            "api\n",
            "document\n",
            "nltk\n",
            "suitabl\n",
            "linguist\n",
            "engin\n",
            "student\n",
            "educ\n",
            "research\n",
            "industri\n",
            "user\n",
            "alik\n",
            "nltk\n",
            "avail\n",
            "window\n",
            "mac\n",
            "os\n",
            "x\n",
            "linux\n",
            "best\n",
            "nltk\n",
            "free\n",
            "open\n",
            "sourc\n",
            "commun\n",
            "driven\n",
            "project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiE-sq5d6uMh",
        "outputId": "bde6d832-6304-4553-f8d4-814e6462987e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk\n",
            "leading\n",
            "platform\n",
            "building\n",
            "python\n",
            "program\n",
            "work\n",
            "human\n",
            "language\n",
            "data\n",
            "provides\n",
            "easy\n",
            "use\n",
            "interface\n",
            "corpus\n",
            "lexical\n",
            "resource\n",
            "wordnet\n",
            "along\n",
            "suite\n",
            "text\n",
            "processing\n",
            "library\n",
            "classification\n",
            "tokenization\n",
            "stemming\n",
            "tagging\n",
            "parsing\n",
            "semantic\n",
            "reasoning\n",
            "wrapper\n",
            "industrial\n",
            "strength\n",
            "nlp\n",
            "library\n",
            "active\n",
            "discussion\n",
            "forum\n",
            "thanks\n",
            "hand\n",
            "guide\n",
            "introducing\n",
            "programming\n",
            "fundamental\n",
            "alongside\n",
            "topic\n",
            "computational\n",
            "linguistics\n",
            "plus\n",
            "comprehensive\n",
            "api\n",
            "documentation\n",
            "nltk\n",
            "suitable\n",
            "linguist\n",
            "engineer\n",
            "student\n",
            "educator\n",
            "researcher\n",
            "industry\n",
            "user\n",
            "alike\n",
            "nltk\n",
            "available\n",
            "window\n",
            "mac\n",
            "o\n",
            "x\n",
            "linux\n",
            "best\n",
            "nltk\n",
            "free\n",
            "open\n",
            "source\n",
            "community\n",
            "driven\n",
            "project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stopwords lemmatize\n",
        "import re\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "  review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  review=review.lower()\n",
        "  review=review.split()\n",
        "  review=[lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review=' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "lehji9Cl9mxJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Bag of words\n",
        " from sklearn.feature_extraction.text import CountVectorizer\n",
        " cv = CountVectorizer(binary=True)\n"
      ],
      "metadata": {
        "id": "-bH1mEiT7qex"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " X = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "PFHNkElz7_B3"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAcRRqfw7_82",
        "outputId": "e41ccf5f-f6f5-492c-bc4e-70f0e9bc6049"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nltk': 39,\n",
              " 'leading': 31,\n",
              " 'platform': 42,\n",
              " 'building': 7,\n",
              " 'python': 49,\n",
              " 'program': 45,\n",
              " 'work': 69,\n",
              " 'human': 25,\n",
              " 'language': 30,\n",
              " 'data': 13,\n",
              " 'provides': 48,\n",
              " 'easy': 17,\n",
              " 'use': 65,\n",
              " 'interface': 28,\n",
              " 'corpus': 12,\n",
              " 'lexical': 32,\n",
              " 'resource': 52,\n",
              " 'wordnet': 68,\n",
              " 'along': 2,\n",
              " 'suite': 59,\n",
              " 'text': 61,\n",
              " 'processing': 44,\n",
              " 'library': 33,\n",
              " 'classification': 8,\n",
              " 'tokenization': 63,\n",
              " 'stemming': 55,\n",
              " 'tagging': 60,\n",
              " 'parsing': 41,\n",
              " 'semantic': 53,\n",
              " 'reasoning': 50,\n",
              " 'wrapper': 70,\n",
              " 'industrial': 26,\n",
              " 'strength': 56,\n",
              " 'nlp': 38,\n",
              " 'active': 0,\n",
              " 'discussion': 14,\n",
              " 'forum': 20,\n",
              " 'thanks': 62,\n",
              " 'hand': 24,\n",
              " 'guide': 23,\n",
              " 'introducing': 29,\n",
              " 'programming': 46,\n",
              " 'fundamental': 22,\n",
              " 'alongside': 3,\n",
              " 'topic': 64,\n",
              " 'computational': 11,\n",
              " 'linguistics': 35,\n",
              " 'plus': 43,\n",
              " 'comprehensive': 10,\n",
              " 'api': 4,\n",
              " 'documentation': 15,\n",
              " 'suitable': 58,\n",
              " 'linguist': 34,\n",
              " 'engineer': 19,\n",
              " 'student': 57,\n",
              " 'educator': 18,\n",
              " 'researcher': 51,\n",
              " 'industry': 27,\n",
              " 'user': 66,\n",
              " 'alike': 1,\n",
              " 'available': 5,\n",
              " 'window': 67,\n",
              " 'mac': 37,\n",
              " 'linux': 36,\n",
              " 'best': 6,\n",
              " 'free': 21,\n",
              " 'open': 40,\n",
              " 'source': 54,\n",
              " 'community': 9,\n",
              " 'driven': 16,\n",
              " 'project': 47}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gF37kYoD8A7T",
        "outputId": "df16709c-29a7-45b1-e77e-ae77e4acbe28"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nltk is a leading platform for building python programs to work with human language data '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DGBX-Yn8LJc",
        "outputId": "e744f5c6-da03-4b1b-f120-b5536fc4c023"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phAEcW-m8OXB"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}